# CompletenessClassifier (Source Code)
## Results and Figures
In the _results and figures_ folder you will find the relevant results and figures discussed in the paper.

## Scripts
In the _src_ folder you will find the code for this project. In the following, brief descriptions of the most relevant scripts and folders are provided
- **kb_construction.py**:
  - implementation of the Knowledge Base enrichment process described in Section 4.1. Eight experiments are executed parallelly
  - for each analyzed column, the corresponding units of knowledge _(column profile, classification method, imputation methods scores)_ are extracted, and saved on a file.
- **Datasets** folder:
  - contains the training and validation datasets used in the project.
  - **dataset_names.txt** contains the list of datasets used in the Knowledge Base Enrichment Process
  - **Arff** folder contains some datasets with .arff extension
  - **CSV** folder contains all used datasets in .csv extension
- **Feature_selection** folder:
  - contains the functions used during feature selection to decide which feature columns of a dataset should be considered in the Knowledge Base enrichment process.
- **Column_profile_extraction** folder:
  - contains the scripts for extracting the numerical and categorical column profiles (**numerical.py** and *categorical.py*)
- **Classification** folder:
  - contains the names of the allowed downstream classification methods and the script to train them on a dataset.
- **Dataset_selection** folder:
  - contains the initial code for selecting the datasets to be used in the Knowledge Base Enrichment process. Note that the list of datasets has been further manually cured.
- **Hyperparameter_tuning** folder:
  - contains a basic hyperparameter tuning for the training of the classification methods on the training datasets. The results will be used during the Knowledge Base enrichment process.
- **Imputation** folder:
  - contains the lists of imputation methods for both numerical and categorical columns
  - contains the implementation of the employed imputation methods (**imputation_techniques.py**)
- **Experiments** folder:
  - **final_files** folder contains the files (with the corresponding seeds) generated by the Knowledge Base enrichment process
  - **combine_new_prova.py** performs the aggregation of the raw experiments in the datasets that will be used to train the specialized classifiers
  - **combined_new_prova** folder contains the aggregated experiment files
  - **binary_classifiers.py** implements and validates the binary classifiers used to predict whether for an unit of knowledge, more or less than four imputation methods are equivalent. The script also produces the partial dependence plots discussed in the paper
- **Classifier** folder:
  - **specialized_classifiers.py** trains, tunes and validates the classifiers used to suggest an imputation method for an incomplete column, given its profile and selected downstream classification method. The script also produces the shapley values plots and swarm plots discussed in the paper.
  - **classifiers** folder contains the trained classifiers, along with the fitted scalers and feature masks.
- **classifier_validation.py**:
  - performs the validation of the specialized classifiers on datasets not used during training
  - it checks the performance of the suggested imputation methods against all possible combinations of methods.
  - the results (figures and performance of all combinations) are saved in the **Classifier_Validation/{validation_dataset_name}** folder.
- **Classifier_Validation** folder:
  - **validate_order_suggestions.py**: allows to make experiments to inspect which is the best ordering of application of the imputation methods suggested for a certain dataset. Different percentages of missing values can be injected in each column to see which ordering of imputation methods is the best.
## Requirements
You can find all the libraries used in this project in the requirements.txt
